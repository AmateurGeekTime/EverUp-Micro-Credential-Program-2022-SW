# -*- coding: utf-8 -*-
"""Copy of Conclusions.Unit 4 Titanic _Logistic_Regression_MLL3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LBZXkrJKWRsoVFHh2en1H4LlPK1oSGs2

# Titanic Dataset Analysis

From Kaggle: The sinking of the Titanic is one of the most infamous shipwrecks in history.

On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.

While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.

In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).

# Tasks:



1.   Every step must have an explanation
2.   Every plot/ chart must have an iterpretation
3.   Add your observations for each section
4.   Draw clear conclusions (at least 5 conclusions)
"""

import numpy as np #importing libarries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

train = pd.read_csv('https://raw.githubusercontent.com/niteen11/cuny_lagcc_micro_credential_data_analytics/main/Track%20A/Unit%204%20-%20Machine%20Learning%20and%20Modeling%20Techniques/Dataset/titanic_train.csv')

train.head() # showing the first data set

train #recalling data table

import pandas as pd #Showing data that is missing 
#show the missing data number
train.isnull().sum()#show the missing data

#show the percentage of missing data  #cleaning up data
100*(train.isnull().sum()/len(train))
def missing_values_percent(train):#we can use this function in all dataframes.
    nan_percent=100*(train.isnull().sum()/len(train))
    nan_percent=nan_percent[nan_percent>0].sort_values()
    return(nan_percent)

nan_percent=missing_values_percent(train)
nan_percent

# Commented out IPython magic to ensure Python compatibility.
#show missing data as a heat map using sns
import seaborn as sns
# %matplotlib inline
import matplotlib.pyplot as plt

sns.heatmap(train.isnull(),cbar=False,  cmap='viridis')

train.drop('Cabin', axis=1, inplace= True)#https://www.w3schools.com/Python/pandas/ref_df_dropna.asp
train.head()
#Drop Cabin column

sns.heatmap(train.isnull(), yticklabels=False,cbar=False,  cmap='viridis')

train["Survived"].value_counts(normalize=False)#How many passengers survived?

train["Survived"].value_counts(normalize=True)#How many passengers survived?

sns.set_style('whitegrid')
sns.countplot(x='Survived', data=train)

train.dropna(inplace=True)
sns.set_style('whitegrid')
sns.countplot(x='Survived', hue='Sex', data=train)

print(train.groupby(["Pclass"])["Survived"].mean().to_frame())
print(train.groupby(["Pclass"])["Survived"].count().to_frame())

sns.set_style('whitegrid')
sns.countplot(x='Survived', hue='Pclass', data=train)

sns.set_style('whitegrid')
sns.countplot(x='Age', hue='Survived', data=train)

#resizing the plot
plt.figure(figsize=(20,5))
sns.set_style('whitegrid')
sns.countplot(x='Age', hue='Survived', data=train)

#Stack chart advanced sns for cs
def bar_chart_stacked(dataset,feature,stacked=True):
  survived=train[train["Survived"]==1][feature].value_counts()
  dead=train[train["Survived"]==0][feature].value_counts()   
  df_survived_dead=pd.DataFrame([survived,dead])  
  df_survived_dead.index=["passengers survived","passengers died"]   
  df_survived_dead.plot(kind="bar",stacked=stacked,figsize=(8,5))

bar_chart_stacked(train,"Pclass")

train.groupby(["Pclass"])["Survived"].mean().to_frame()

"""We see that 62% of passengers in class 1 were survived but this amount is reduced to 47% for class 2 and only 24% of passengers in class 3 were survived.On the other words the percentage of survived passengers in class 1 is 2 times bigger than the percentage of died passengers in this class.But in class 2 the percentage of survived people and died people is almost equal and for class 3 the percentage of died people is three times bigger than survived one."""

bar_chart_stacked(train,"Survived")

bar_chart_stacked(train,"Sex")#compare the survived  and dead passengers counts on gender

bar_chart_stacked(train,"Pclass")

def bar_chart_compare(dataset,feature1,feature2=None):
    plt.figure(figsize=(8,5))
    plt.title("survived rate by sex and pclass")
    g=sns.barplot(x=feature1,y="Survived",hue=feature2,data=dataset).set

bar_chart_compare(train,"Pclass","Sex")

"""We see that the number of men and women who were survived is decreasing according to class.In addition,men and women in class 1 had a significantly higher chance of survival if they bought class 1 tickets."""

#https://www.dataindependent.com/pandas/pandas-get-dummies/
sex = pd.get_dummies(train['Sex'],drop_first=True)
embark = pd.get_dummies(train['Embarked'],drop_first=True)
train.head()

train.drop(['Sex','Embarked','Name','Ticket'], axis=1, inplace= True)

train.head()

# Concatenate pandas objects along a particular axis 
train = pd.concat([train, sex, embark],axis=1)
train.head()

"""# Data Cleaning and Data Pre - Processing

## Class
"""

sns.set_style('whitegrid')
sns.countplot(x='Survived', data=train)

"""## Survived"""

#sns.set_style('whitegrid')
#sns.countplot(x='Survived', hue='Sex', data=train)

sns.set_style('whitegrid')
sns.countplot(x='Survived', hue='Pclass', data=train)

"""## Age"""

sns.distplot(train['Age'],kde=True, bins=30)

"""## Siblings"""

sns.countplot(x='SibSp',data=train)



train.head()



"""# Model building"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(train.drop('Survived',axis=1), train['Survived'],test_size=0.3, random_state=101)

from sklearn.linear_model import LogisticRegression

logmodel = LogisticRegression()

#Traning the Logistic regression model with  traning Data
logmodel.fit(X_train, y_train)

predict = logmodel.predict(X_test)

"""Model Evaluation

Accuracy score

## Classification report
A Classification report is used to measure the quality of predictions from a classification algorithm
"""

from sklearn.metrics import classification_report

print(classification_report(y_test,predict))

"""# There are four ways to check if the predictions are right or wrong:


TN / True Negative: the case was negative and predicted negative (Did not survived and pridicted that will not survive)

TP / True Positive: the case was positive and predicted positive (survived and pridicted that will survive)

FN / False Negative: the case was positive but predicted negative (survived but predicted will not

FP / False Positive: the case was negative but predicted positive Predicted to survive but did not) 


<h1>Precision — **</h1>







# What percent of your predictions were correct?

Precision is the ability of a classifier not to label an instance positive that is actually negative. For each class, it is defined as the ratio of true positives to the sum of a true positive and false positive.

# Precision:- Accuracy of positive predictions.
Precision = TP/(TP + FP)

### Recall — What percent of the positive cases did you catch?
Recall is the ability of a classifier to find all positive instances. For each class it is defined as the ratio of true positives to the sum of true positives and false negatives.

Recall:- Fraction of positives that were correctly identified.

Recall = TP/(TP+FN)

F1 score — What percent of positive predictions were correct?
The F1 score is a weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0. F1 scores are lower than accuracy measures as they embed precision and recall into their computation. As a rule of thumb, the weighted average of F1 should be used to compare classifier models, not global accuracy.

F1 Score = 2*(Recall * Precision) / (Recall + Precision)

# Support

Support is the number of actual occurrences of the class in the specified dataset. Imbalanced support in the training data may indicate structural weaknesses in the reported scores of the classifier and could indicate the need for stratified sampling or rebalancing. Support doesn’t change between models but instead diagnoses the evaluation process.

## Confusion Matrix
A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model,of (y_test,predict)
"""

from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test, predict))
print("Total of 713 cases 30% of the test sett  is ,  214=105+60+26+23")

"""True positive: 105 (We predicted a positive result and it was positive)

True negative: 60 (We predicted a negative result and it was negative)

False positive: 23 (We predicted a positive result and it was negative)

False negative: 26 (We predicted a negative result and it was positive)

<img src="https://glassboxmedicine.files.wordpress.com/2019/02/confusion-matrix.png?w=816">
<h4>Total of 713 cases 30% of the test sett  is ,  214=105+60+26+23</h4>
"""

plt.figure(figsize=(9,9))
sns.heatmap(confusion_matrix(y_test, predict), annot=True, fmt=".3f", linewidths=.5, square = True, cmap = 'Blues_r');
plt.ylabel('Actual label');
plt.xlabel('Predicted label');
all_sample_title = 'Accuracy Score: {0}'.format(logmodel.score(X_test, y_test))
plt.title(all_sample_title, size = 15);

from sklearn import  metrics

metrics.plot_roc_curve(logmodel, X_test, y_test)

"""Observation..
The accuracy is high from the data visualizations.

<h1># Conclusion...</h1>

Accodring to the survival bar graph, there are few groups of people more likely to be survived. These include women, people around the age range of 20s and first-class passengers.
"""