# -*- coding: utf-8 -*-
"""Copy of Copy of Machine Learning - Linear Regression - Guided Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yz3UpZFO8cI0_-pXxyZqw5A9jzAc-XrL

# Machine Learning - Linear Regression on Boston Housing Dataset

## Data Background and Problem Statement
https://towardsdatascience.com/linear-regression-on-boston-housing-dataset-f409b7e4a155

We will take the Housing dataset which contains information about different houses in Boston. This data was originally a part of UCI Machine Learning Repository and has been removed now. We can also access this data from the scikit-learn library. There are 506 samples and 13 feature variables in this dataset. The objective is to predict the value of prices of the house using the given features.Open-source ML library for Python. Built on NumPy, SciPy, and Matplotlib. ... Scikit-learn is a library in Python that provides many unsupervised and supervised learning algorithms.

# Task 1 : Environment Set up
"""

#import required libraries
import numpy as np
import pandas as pd

"""# Task 2 : Data Collection"""

# import the boston dataset
from sklearn.datasets import load_boston
boston_dataset = load_boston()

# create a pandas dataframe and store the data
df_boston = pd.DataFrame(boston_dataset.data)
df_boston.columns = boston_dataset.feature_names
df_boston.columns

df_boston

# append Price, target, as a new columnn to the dataset
df_boston['Price'] = boston_dataset.target

# print top 5 observations
df_boston.head()

"""We want first to check to see if we have missing data ,we use isnull().sum()"""

df_boston.isnull().sum() #sum to count of the NaN values For one column

df_boston["Price"].mean()

"""# Task 3 : Data Wrangling and EDA (Exploratory Data Analysis)"""

import seaborn as sns
import matplotlib.pyplot as plt

#sns.set(rc={'figure.figsize':(12,12)})#. rc parameter set aesthetics of your plots 
# set figure size by passing dictionary to rc parameter with key 'figure.figsize' in seaborn set method:
sns.distplot(df_boston['Price'], bins=20)
plt.show()
sns.histplot(df_boston['Price'], kde = True)
plt.show()

"""#  Create a correlation matrix that measures the linear relationships between the variables
# A perfect negative correlation is represented by the value -1.0, while a 0 indicates no correlation, and +1.0 indicates a perfect positive correlation. A perfect negative correlation means the relationship that exists between two variables is exactly opposite all of the time.

"""

correlation_matrix = df_boston.corr().round(2)
#A correlation matrix is a table showing correlation coefficients between variables.
# annot = True to print the values inside the square
sns.heatmap(data=correlation_matrix,annot=True)# put the numbers on
sns.set(rc = {'figure.figsize':(19,13)})



"""# Write Your Observations
CRIM and Price is a positive correlation by +1.0

# Preparing the data for training the Machine Learning Model
"""



# assign features on X axis 
X_features = boston_dataset.data

# assign target on Y axis 
Y_target = boston_dataset.target

"""# Build Linear Regression Model"""



# import linear model - the estimator
from sklearn.linear_model import LinearRegression
lineReg = LinearRegression()

# fit data into the the estimator
lineReg.fit(X_features,Y_target)

# The simple linear regression model is essentially a linear equation of the form y = c + b*x; 
#where y is the dependent variable (outcome), x is the independent variable (predictor), b is the
# slope of the line;
# also known as regression coefficient and c is the intercept; labeled as constant.
print('the estimated intercept %.2f '%lineReg.intercept_)

# Regression coefficients The Regression Coefficient is the constant 'b' in the regression equation 
#that tells about the change in the value of dependent variable corresponding to the unit change in 
#the independent variable.y = -3.6 + 5.0X 1 - 1.8X 2, the variables X 1 and X 2 are multiplied by 5.0 and -1.8, 
#respectively, so the coefficients are 5.0 and -1.8
# print the coefficient 
print('the coefficient is %d ' %len(lineReg.coef_))

"""# Model Training"""

# train model split the whole dataset into train and test datasets
from sklearn.model_selection import train_test_split
#X_train, X_test, Y_train, Y_test = train_test_split(X_features,Y_target)
X_train, X_test, Y_train, Y_test = train_test_split(X_features,Y_target, train_size=0.80, test_size = 0.2, random_state=15)

# print the dataset shape
print(boston_dataset.data.shape)

# Print shapes of the training and testing data sets
print( X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)

# fit the training sets into the model
lineReg.fit(X_train,Y_train)

"""# Caluclate RMSE and R Square:

# RMSE
Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit. Root mean square error is commonly used in climatology, forecasting, and regression analysis to verify experimental results.

# R Square

R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination,the higher r means ,the higher percentage of points the line passes through when the data points and line are plotted. If the coefficient is 0.80, then 80% of the points should fall within the regression line.The usefulness of R2 is its ability to find the likelihood of future events falling within the predicted outcomes. 


https://drive.google.com/file/d/1YPzOS2dbKIuVOuYWhCeP9HOs0WcWxGaK/view?usp=sharing

"""

from sklearn.metrics import mean_squared_error, r2_score

y_train_predict = lineReg.predict(X_train)

rmse = np.sqrt(mean_squared_error(Y_train,y_train_predict))

#The usefulness of R2 is its ability to find the likelihood of future
# events falling within the predicted outcomes.
r2 = r2_score(Y_train, y_train_predict)
print("The model performance for training set")
print("--------------------------------------")
#RMSE is a measure of how spread out these residuals are.
print('RMSE is {}'.format(rmse))
print('R2 score is {}'.format(r2))
print("\n")

# model evaluation for testing set

y_test_predict = lineReg.predict(X_test)
rmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))
r2 = r2_score(Y_test, y_test_predict)

print("The model performance for testing set")
print("--------------------------------------")
print('RMSE is {}'.format(rmse))
print('R2 score is {}'.format(r2))

import pandas as pd
import seaborn as sns
# plotting the y_test vs y_pred
# ideally should have been a straight line

plt.xlabel('Actuals')
plt.ylabel('Predicted Values')
plt.title('Actuals Vs Predicted Values')
scatter=plt.scatter(Y_test, y_test_predict)
regplot=sns.regplot(Y_test, y_test_predict,x="Y_test", y="y_test_predict", data=scatter,
                 scatter_kws={"color": "black"}, line_kws={"color": "red"})

"""# Your Conclusion

To conclude the Training set has a RMSE of 4.6767 and a R-Squared of 0.7455 while the testing set has a RMSE of 4.8798 and a R-Squared of 0.6921; we can make an assumptions that the data are quite precise but not so accurate.
"""